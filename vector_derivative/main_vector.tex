\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{Vector Derivative For Back Propagation}
\author{Yongqiang Huang}
\date{December 2019}

\begin{document}

\maketitle

\section{Jacobian and gradient}
In this article we go through the basic vector calculus we have needed \emph{so far} for computing back propagation for neural networks. First we need to familiarize ourselves with the concept of Jacobian, which is a container that holds derivative of each output variable with respect to each input variable. To be specific, let a function be
\begin{equation}
    y = f(x)
\end{equation}
where $f: \mathbb{R}^N \rightarrow \mathbb{R}^M$, then the Jacobian matrix is defined as 
\begin{equation}
    J = \begin{bmatrix}
    \frac{\partial y_1}{\partial x_1}& \frac{\partial y_1}{\partial x_2}& \cdots & \frac{\partial y_1}{\partial x_N} \\
    \frac{\partial y_2}{\partial x_1}& \frac{\partial y_2}{\partial x_2}& \cdots & \frac{\partial y_2}{\partial x_N} \\
    \vdots & \vdots & \vdots & \vdots \\
    \frac{\partial y_M}{\partial x_1}& \frac{\partial y_M}{\partial x_2}& \cdots & \frac{\partial y_M}{\partial x_N} \\
    \end{bmatrix}
\end{equation}
In this case, the dimension of $J$ is $M\times N$. If $f: \mathbb{R}^N \rightarrow \mathbb{R}$, then $J$ has dimension $1\times N$: only one $y$ corresponding to $N$ $x_i$'s. If $f: \mathbb{R} \rightarrow \mathbb{R}^M$, then $J$ has dimension $M\times 1$. Basically
\begin{equation}
    J_{i, j} = \frac{\partial f(x)_i}{\partial x_j}
\end{equation}
Gradient is one level down from Jacobian. We usually consider gradient $\nabla_{x}f(x)$ as the derivative of a function $f: \mathbb{R}^N \rightarrow \mathbb{R}$. The gradient matches in dimension the input $x$. If $x$ has dimension $N\times 1$, $\nabla_{x}f(x)$ has dimension $N\times 1$. If $x$ has dimension $N\times M$, $\nabla_{x}f(x)$ has dimension $N\times M$. 
In neural networks, we define a loss function $L$ which is a scalar, and we compute the gradient of $L$ with respect to the weight matrices, $\frac{\partial L}{\partial W}$. Usually $W$ is a matrix, say of dimension $M\times N$, and the dimension of the gradient would be $M\times N$. 

\section{$y = Wx$}
Let's look at a simple neural network
\begin{equation}
    y = Wx
\end{equation}
for which the dimensions are
\begin{align}
    y &: M \times 1 \\
    x &: N \times 1 \\
    W &: M \times N
\end{align}
Suppose some loss function $L$ is defined which is a function of $y$ and somehow you have computed the gradient with respect to $y$:
\begin{equation}
    \nabla_yL = \frac{\partial L}{\partial y} = \delta
\end{equation}
Note that here we use $\nabla_yL$ and $\frac{\partial L}{\partial y}$ interchangably, which may not be very rigorous. The dimension of $\frac{\partial L}{\partial y}$ should be the same as that of $y$: $M \times 1$. Now here are two questions: given $\delta$, what is $\frac{\partial L}{\partial x}$ and what is $\frac{\partial L}{\partial W}$? We use chain rule to solve them. First we look at $\frac{\partial L}{\partial x}$ which is computed by
\begin{equation}
    \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x},
\end{equation}
and what is $\frac{\partial y}{\partial x}$? $\frac{\partial y}{\partial x}$ is a Jacobian matrix. To know what its elements are, we need to take a closer look at how $y$ is calculated from $x$:
\begin{equation} \label{eq-xy}
    y_i = \sum_{k=1}^NW_{i, k}x_k
\end{equation}
What you want to notice that row $i$ in $y$, or actually $y_i$ only has relations with row $i$ in $W$, and has nothing to do with any other row in $W$. Thus, 
\begin{equation}
    \frac{\partial y_i}{\partial x_j} = W_{i, j}
\end{equation}
Which written in vector form is
\begin{equation}
    \frac{\partial y}{\partial x} = W
\end{equation}
Notice that we always \emph{first} look at the element of a gradient and \emph{then} recover its vector form. The vector form is neat, but we may not need it for \emph{every} step of the chain rule. We do need it for certain steps, i.e. for the inbound and outbound derivative for any operation. Now let's go back to the gradient with respect to $x$: $\frac{\partial L}{\partial x}$. We already know the shape of $\frac{\partial L}{\partial x}$, which is $N \times 1$. To determine its entirety, we really only need its elements:
\begin{equation}
    \frac{\partial L}{\partial x_i} = \sum_{k=1}^M\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_i}
\end{equation}
Pay attention to this step. $L$ is a function of $\{y_1, y_2, \dots, y_M\}$, so the gradient with respect to $x_i$ must go through every $y_k$. Let's get back to the formula
\begin{align}
    \frac{\partial L}{\partial x_i} = \sum_{k=1}^M\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_i} = \sum_{k=1}^M\frac{\partial L}{\partial y_k}W_{k, i}
\end{align}
which is an inner product between $\delta$ and the $i$-th column of $W$. Its vector form (we actually need the vector form this time) is:
\begin{equation}
    \frac{\partial L}{\partial x} 
    = W^\top\frac{\partial L}{\partial y} = W^\top\delta
\end{equation}
If we look at the dimension, we get: $(N\times M) \times (M \times 1)=N \times 1$ which is compatible with $\frac{\partial L}{\partial x}$. 
Now let's compute $\frac{\partial L}{\partial W}$, which is a little bit more difficult:
\begin{equation}
    \frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W}
\end{equation}
$\frac{\partial y}{\partial W}$ is, believe it or not, a Jacobian, which has a dimension of $M\times(M\times N)$. This is the case when we do \emph{not} want to recover its vector form: it's a tensor, and if we simply expand it, we might make mistakes easily. Actually, we don't need to expand it, we just need to know its elements: $\frac{\partial y}{\partial W_{i, j}}$. Again:
\begin{equation}
    y_i = \sum_{j=1}^NW_{i, j}x_j
\end{equation}
and therefore
\begin{equation}
    \frac{\partial y_k}{\partial W_{i, j}} = \mathbf{1}(k=i)x_j
\end{equation}
Thus
\begin{align}
    \frac{\partial L}{\partial W_{i, j}} 
    &= \sum_{k=1}^M\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial W_{i, j}} \\
    &= \sum_{k=1}^M\frac{\partial L}{\partial y_k}\mathbf{1}(k=i)x_j \\
    &= \frac{\partial L}{\partial y_i}x_j
\end{align}
The restored vector form is
\begin{equation}
    \frac{\partial L}{\partial W} = \delta x^\top 
\end{equation}
Let's check the dimension: $(M\times 1) \times (1 \times N) = M \times N$, compatible with $W$. 

\section{$y = xW$}
We can consider the same network but formulate it a little bit differently:
\begin{equation}
    y = xW
\end{equation}
for which the dimensions are:
\begin{align}
    y &: 1 \times M \\
    x &: 1 \times N \\
    W &: N \times M
\end{align}
All we do is transpose the input $x$ really, and $W$ and $y$ accordingly. Assume, again, that you have computed $\frac{\partial L}{\partial y} = \delta$ which now has dimension $1\times M$. Again, we want to compute $\frac{\partial L}{\partial x}$, $\frac{\partial L}{\partial W}$, and we will follow almost the same procedure as we do for $x$ being a column vector. We need two immediate partial derivatives: $\frac{\partial y_k}{\partial x_i}$ and $\frac{\partial y_k}{\partial W_{i, j}}$. Again, we need to know how $y$ is computed from $x$:
\begin{equation}
    y_k = \sum_{i = 1}^N x_iW_{i, k}
\end{equation}
Thus 
\begin{equation}
    \frac{\partial y_k}{\partial x_i} = W_{i, k} 
\end{equation}
which restored to vector form is
\begin{equation}
    \frac{\partial y}{\partial x} = W^\top
\end{equation}
and
\begin{align}
   \frac{\partial L}{\partial x_i} 
   &= \sum_{k=1}^M\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_i} = \sum_{k=1}^M\frac{\partial L}{\partial y_k}W_{i, k} 
\end{align}
which restored to vector form is
\begin{equation}
    \frac{\partial L}{\partial x} = \delta W^\top
\end{equation}
Let's check the dimension: $(1\times M) \times (M \times N) = 1 \times N$, compatible with $x$.

Next we compute $\frac{\partial L}{\partial W}$. First, 
\begin{equation}
    \frac{\partial y_k}{\partial W_{i, j}} = \mathbf{1}(k=j)x_i
\end{equation}
Then
\begin{align}
    \frac{\partial L}{\partial W_{i, j}} 
    &= \sum_{k=1}^M\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial W_{i, j}} \\ 
    &= \sum_{k=1}^M\frac{\partial L}{\partial y_k}\mathbf{1}(k=j)x_i \\
    &= \frac{\partial L}{\partial y_j}x_i
\end{align}
which restored to vector form is
\begin{equation}
    \frac{\partial L}{\partial W} = x^\top\delta
\end{equation}
The dimension is $(N\times 1) \times (1 \times M) = N \times M$, which is compatible with $W$.

\section{Y = XW}

\end{document}
